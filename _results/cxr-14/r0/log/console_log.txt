INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:=> init weight of Linear from trunc norm
INFO:root:=> init bias of Linear to zeros
INFO:root:Region2RegionNet(
  (context_encoder): R2RConvolutionalVisionTransformer(
    (stage0): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=64, out_features=64, bias=True)
            (proj_k): Linear(in_features=64, out_features=64, bias=True)
            (proj_v): Linear(in_features=64, out_features=64, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage1): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0-1): 2 x Block(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
              (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=192, out_features=192, bias=True)
            (proj_k): Linear(in_features=192, out_features=192, bias=True)
            (proj_v): Linear(in_features=192, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (stage2): VisionTransformer(
      (patch_embed): ConvEmbed(
        (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): ModuleList(
        (0): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.011)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.022)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.044)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.056)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.089)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): Block(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (conv_proj_q): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_k): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (conv_proj_v): Sequential(
              (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)
              (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (rearrage): Rearrange('b c h w -> b (h w) c')
            )
            (proj_q): Linear(in_features=384, out_features=384, bias=True)
            (proj_k): Linear(in_features=384, out_features=384, bias=True)
            (proj_v): Linear(in_features=384, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (last_layer): Linear(in_features=384, out_features=14, bias=True)
)
INFO:root:[  0/100] T  99/2704 [WeightedBalanceLoss: 4.0116][loss: 4.0116][auc: 0.5589]
INFO:root:[  0/100] T 199/2704 [WeightedBalanceLoss: 3.4906][loss: 3.4906][auc: 0.5774]
INFO:root:[  0/100] T 299/2704 [WeightedBalanceLoss: 2.6947][loss: 2.6947][auc: 0.6021]
INFO:root:[  0/100] T 399/2704 [WeightedBalanceLoss: 2.7116][loss: 2.7116][auc: 0.6230]
INFO:root:[  0/100] T 499/2704 [WeightedBalanceLoss: 2.6782][loss: 2.6782][auc: 0.6355]
INFO:root:[  0/100] T 599/2704 [WeightedBalanceLoss: 2.5947][loss: 2.5947][auc: 0.6467]
INFO:root:[  0/100] T 699/2704 [WeightedBalanceLoss: 3.1881][loss: 3.1881][auc: 0.6528]
INFO:root:[  0/100] T 799/2704 [WeightedBalanceLoss: 2.4513][loss: 2.4513][auc: 0.6582]
INFO:root:[  0/100] T 899/2704 [WeightedBalanceLoss: 2.7927][loss: 2.7927][auc: 0.6635]
INFO:root:[  0/100] T 999/2704 [WeightedBalanceLoss: 2.6178][loss: 2.6178][auc: 0.6683]
INFO:root:[  0/100] T 1099/2704 [WeightedBalanceLoss: 2.9115][loss: 2.9115][auc: 0.6747]
INFO:root:[  0/100] T 1199/2704 [WeightedBalanceLoss: 2.6769][loss: 2.6769][auc: 0.6810]
INFO:root:[  0/100] T 1299/2704 [WeightedBalanceLoss: 2.6227][loss: 2.6227][auc: 0.6854]
INFO:root:[  0/100] T 1399/2704 [WeightedBalanceLoss: 3.2454][loss: 3.2454][auc: 0.6914]
INFO:root:[  0/100] T 1499/2704 [WeightedBalanceLoss: 2.9977][loss: 2.9977][auc: 0.6963]
INFO:root:[  0/100] T 1599/2704 [WeightedBalanceLoss: 2.4949][loss: 2.4949][auc: 0.7013]
INFO:root:[  0/100] T 1699/2704 [WeightedBalanceLoss: 3.3562][loss: 3.3562][auc: 0.7044]
INFO:root:[  0/100] T 1799/2704 [WeightedBalanceLoss: 2.9505][loss: 2.9505][auc: 0.7072]
INFO:root:[  0/100] T 1899/2704 [WeightedBalanceLoss: 3.0544][loss: 3.0544][auc: 0.7126]
INFO:root:[  0/100] T 1999/2704 [WeightedBalanceLoss: 2.3113][loss: 2.3113][auc: 0.7152]
INFO:root:[  0/100] T 2099/2704 [WeightedBalanceLoss: 2.8948][loss: 2.8948][auc: 0.7188]
INFO:root:[  0/100] T 2199/2704 [WeightedBalanceLoss: 2.6518][loss: 2.6518][auc: 0.7213]
INFO:root:[  0/100] T 2299/2704 [WeightedBalanceLoss: 2.6222][loss: 2.6222][auc: 0.7239]
INFO:root:[  0/100] T 2399/2704 [WeightedBalanceLoss: 2.8133][loss: 2.8133][auc: 0.7264]
INFO:root:[  0/100] T 2499/2704 [WeightedBalanceLoss: 3.0067][loss: 3.0067][auc: 0.7291]
INFO:root:[  0/100] T 2599/2704 [WeightedBalanceLoss: 2.8084][loss: 2.8084][auc: 0.7312]
INFO:root:[  0/100] T 2699/2704 [WeightedBalanceLoss: 2.1849][loss: 2.1849][auc: 0.7336]
INFO:root:[Epoh 0] Train [auc:0.7337] [t_time: 1129.31]
INFO:root:[  0/100] V  99/800 [WeightedBalanceLoss: 4.2398][loss: 4.2398][auc: 0.7391]
INFO:root:[  0/100] V 199/800 [WeightedBalanceLoss: 3.2529][loss: 3.2529][auc: 0.7397]
INFO:root:[  0/100] V 299/800 [WeightedBalanceLoss: 4.0099][loss: 4.0099][auc: 0.7439]
INFO:root:[  0/100] V 399/800 [WeightedBalanceLoss: 3.7541][loss: 3.7541][auc: 0.7463]
INFO:root:[  0/100] V 499/800 [WeightedBalanceLoss: 3.9105][loss: 3.9105][auc: 0.7457]
INFO:root:[  0/100] V 599/800 [WeightedBalanceLoss: 3.3916][loss: 3.3916][auc: 0.7477]
INFO:root:[  0/100] V 699/800 [WeightedBalanceLoss: 1.7648][loss: 1.7648][auc: 0.7469]
INFO:root:[  0/100] V 799/800 [WeightedBalanceLoss: 0.6899][loss: 0.6899][auc: 0.7583]
INFO:root:[Epoh 0] Test [auc:0.7583] [v_time: 275.43]
INFO:root:[  1/100] T  99/2704 [WeightedBalanceLoss: 3.3326][loss: 3.3326][auc: 0.7745]
INFO:root:[  1/100] T 199/2704 [WeightedBalanceLoss: 2.8453][loss: 2.8453][auc: 0.7779]
INFO:root:[  1/100] T 299/2704 [WeightedBalanceLoss: 3.1941][loss: 3.1941][auc: 0.7794]
INFO:root:[  1/100] T 399/2704 [WeightedBalanceLoss: 2.7485][loss: 2.7485][auc: 0.7809]
INFO:root:[  1/100] T 499/2704 [WeightedBalanceLoss: 2.5075][loss: 2.5075][auc: 0.7836]
INFO:root:[  1/100] T 599/2704 [WeightedBalanceLoss: 3.1272][loss: 3.1272][auc: 0.7865]
INFO:root:[  1/100] T 699/2704 [WeightedBalanceLoss: 2.6425][loss: 2.6425][auc: 0.7859]
INFO:root:[  1/100] T 799/2704 [WeightedBalanceLoss: 3.4775][loss: 3.4775][auc: 0.7892]
INFO:root:[  1/100] T 899/2704 [WeightedBalanceLoss: 3.1402][loss: 3.1402][auc: 0.7902]
INFO:root:[  1/100] T 999/2704 [WeightedBalanceLoss: 3.0800][loss: 3.0800][auc: 0.7915]
INFO:root:[  1/100] T 1099/2704 [WeightedBalanceLoss: 2.2070][loss: 2.2070][auc: 0.7924]
INFO:root:[  1/100] T 1199/2704 [WeightedBalanceLoss: 4.8800][loss: 4.8800][auc: 0.7932]
INFO:root:[  1/100] T 1299/2704 [WeightedBalanceLoss: 1.9753][loss: 1.9753][auc: 0.7937]
INFO:root:[  1/100] T 1399/2704 [WeightedBalanceLoss: 2.7696][loss: 2.7696][auc: 0.7947]
INFO:root:[  1/100] T 1499/2704 [WeightedBalanceLoss: 2.6457][loss: 2.6457][auc: 0.7947]
INFO:root:[  1/100] T 1599/2704 [WeightedBalanceLoss: 3.3166][loss: 3.3166][auc: 0.7961]
INFO:root:[  1/100] T 1699/2704 [WeightedBalanceLoss: 2.1216][loss: 2.1216][auc: 0.7973]
INFO:root:[  1/100] T 1799/2704 [WeightedBalanceLoss: 2.8548][loss: 2.8548][auc: 0.7974]
INFO:root:[  1/100] T 1899/2704 [WeightedBalanceLoss: 2.6915][loss: 2.6915][auc: 0.7969]
INFO:root:[  1/100] T 1999/2704 [WeightedBalanceLoss: 2.6265][loss: 2.6265][auc: 0.7969]
INFO:root:[  1/100] T 2099/2704 [WeightedBalanceLoss: 3.3580][loss: 3.3580][auc: 0.7972]
INFO:root:[  1/100] T 2199/2704 [WeightedBalanceLoss: 2.5764][loss: 2.5764][auc: 0.7982]
INFO:root:[  1/100] T 2299/2704 [WeightedBalanceLoss: 2.5497][loss: 2.5497][auc: 0.7993]
INFO:root:[  1/100] T 2399/2704 [WeightedBalanceLoss: 2.4839][loss: 2.4839][auc: 0.7996]
INFO:root:[  1/100] T 2499/2704 [WeightedBalanceLoss: 2.3523][loss: 2.3523][auc: 0.7999]
INFO:root:[  1/100] T 2599/2704 [WeightedBalanceLoss: 3.5308][loss: 3.5308][auc: 0.8001]
INFO:root:[  1/100] T 2699/2704 [WeightedBalanceLoss: 2.9999][loss: 2.9999][auc: 0.8004]
INFO:root:[Epoh 1] Train [auc:0.8004] [t_time: 1140.58]
INFO:root:[  1/100] V  99/800 [WeightedBalanceLoss: 3.7814][loss: 3.7814][auc: 0.7634]
INFO:root:[  1/100] V 199/800 [WeightedBalanceLoss: 3.0845][loss: 3.0845][auc: 0.7655]
INFO:root:[  1/100] V 299/800 [WeightedBalanceLoss: 3.7246][loss: 3.7246][auc: 0.7714]
INFO:root:[  1/100] V 399/800 [WeightedBalanceLoss: 3.5334][loss: 3.5334][auc: 0.7731]
INFO:root:[  1/100] V 499/800 [WeightedBalanceLoss: 3.4026][loss: 3.4026][auc: 0.7723]
INFO:root:[  1/100] V 599/800 [WeightedBalanceLoss: 3.3539][loss: 3.3539][auc: 0.7732]
INFO:root:[  1/100] V 699/800 [WeightedBalanceLoss: 1.6922][loss: 1.6922][auc: 0.7736]
INFO:root:[  1/100] V 799/800 [WeightedBalanceLoss: 0.7225][loss: 0.7225][auc: 0.7830]
INFO:root:[Epoh 1] Test [auc:0.7830] [v_time: 68.21]
INFO:root:[  2/100] T  99/2704 [WeightedBalanceLoss: 3.4395][loss: 3.4395][auc: 0.8206]
INFO:root:[  2/100] T 199/2704 [WeightedBalanceLoss: 3.1095][loss: 3.1095][auc: 0.8228]
INFO:root:[  2/100] T 299/2704 [WeightedBalanceLoss: 2.2492][loss: 2.2492][auc: 0.8217]
INFO:root:[  2/100] T 399/2704 [WeightedBalanceLoss: 2.4648][loss: 2.4648][auc: 0.8215]
INFO:root:[  2/100] T 499/2704 [WeightedBalanceLoss: 3.6168][loss: 3.6168][auc: 0.8236]
INFO:root:[  2/100] T 599/2704 [WeightedBalanceLoss: 1.9958][loss: 1.9958][auc: 0.8213]
INFO:root:[  2/100] T 699/2704 [WeightedBalanceLoss: 2.3240][loss: 2.3240][auc: 0.8230]
INFO:root:[  2/100] T 799/2704 [WeightedBalanceLoss: 2.7665][loss: 2.7665][auc: 0.8244]
INFO:root:[  2/100] T 899/2704 [WeightedBalanceLoss: 3.0644][loss: 3.0644][auc: 0.8244]
INFO:root:[  2/100] T 999/2704 [WeightedBalanceLoss: 2.7625][loss: 2.7625][auc: 0.8237]
INFO:root:[  2/100] T 1099/2704 [WeightedBalanceLoss: 2.5394][loss: 2.5394][auc: 0.8223]
INFO:root:[  2/100] T 1199/2704 [WeightedBalanceLoss: 2.4258][loss: 2.4258][auc: 0.8218]
INFO:root:[  2/100] T 1299/2704 [WeightedBalanceLoss: 3.0592][loss: 3.0592][auc: 0.8221]
INFO:root:[  2/100] T 1399/2704 [WeightedBalanceLoss: 2.6234][loss: 2.6234][auc: 0.8214]
INFO:root:[  2/100] T 1499/2704 [WeightedBalanceLoss: 2.5188][loss: 2.5188][auc: 0.8227]
INFO:root:[  2/100] T 1599/2704 [WeightedBalanceLoss: 1.7919][loss: 1.7919][auc: 0.8230]
INFO:root:[  2/100] T 1699/2704 [WeightedBalanceLoss: 2.2089][loss: 2.2089][auc: 0.8234]
INFO:root:[  2/100] T 1799/2704 [WeightedBalanceLoss: 2.3387][loss: 2.3387][auc: 0.8232]
INFO:root:[  2/100] T 1899/2704 [WeightedBalanceLoss: 2.8976][loss: 2.8976][auc: 0.8232]
INFO:root:[  2/100] T 1999/2704 [WeightedBalanceLoss: 3.2918][loss: 3.2918][auc: 0.8228]
INFO:root:[  2/100] T 2099/2704 [WeightedBalanceLoss: 2.8718][loss: 2.8718][auc: 0.8233]
INFO:root:[  2/100] T 2199/2704 [WeightedBalanceLoss: 2.8787][loss: 2.8787][auc: 0.8229]
INFO:root:[  2/100] T 2299/2704 [WeightedBalanceLoss: 2.6514][loss: 2.6514][auc: 0.8226]
INFO:root:[  2/100] T 2399/2704 [WeightedBalanceLoss: 2.4347][loss: 2.4347][auc: 0.8226]
INFO:root:[  2/100] T 2499/2704 [WeightedBalanceLoss: 2.4098][loss: 2.4098][auc: 0.8230]
INFO:root:[  2/100] T 2599/2704 [WeightedBalanceLoss: 2.2757][loss: 2.2757][auc: 0.8229]
INFO:root:[  2/100] T 2699/2704 [WeightedBalanceLoss: 2.8615][loss: 2.8615][auc: 0.8226]
INFO:root:[Epoh 2] Train [auc:0.8225] [t_time: 1094.95]
INFO:root:[  2/100] V  99/800 [WeightedBalanceLoss: 3.6096][loss: 3.6096][auc: 0.7640]
INFO:root:[  2/100] V 199/800 [WeightedBalanceLoss: 3.0129][loss: 3.0129][auc: 0.7681]
INFO:root:[  2/100] V 299/800 [WeightedBalanceLoss: 3.8433][loss: 3.8433][auc: 0.7728]
INFO:root:[  2/100] V 399/800 [WeightedBalanceLoss: 3.5185][loss: 3.5185][auc: 0.7741]
INFO:root:[  2/100] V 499/800 [WeightedBalanceLoss: 3.6468][loss: 3.6468][auc: 0.7744]
INFO:root:[  2/100] V 599/800 [WeightedBalanceLoss: 3.2242][loss: 3.2242][auc: 0.7768]
INFO:root:[  2/100] V 699/800 [WeightedBalanceLoss: 1.9288][loss: 1.9288][auc: 0.7766]
INFO:root:[  2/100] V 799/800 [WeightedBalanceLoss: 0.6209][loss: 0.6209][auc: 0.7864]
INFO:root:[Epoh 2] Test [auc:0.7864] [v_time: 68.91]
INFO:root:[  3/100] T  99/2704 [WeightedBalanceLoss: 2.2815][loss: 2.2815][auc: 0.8418]
INFO:root:[  3/100] T 199/2704 [WeightedBalanceLoss: 2.9913][loss: 2.9913][auc: 0.8326]
INFO:root:[  3/100] T 299/2704 [WeightedBalanceLoss: 2.1645][loss: 2.1645][auc: 0.8312]
INFO:root:[  3/100] T 399/2704 [WeightedBalanceLoss: 2.8311][loss: 2.8311][auc: 0.8348]
INFO:root:[  3/100] T 499/2704 [WeightedBalanceLoss: 2.2780][loss: 2.2780][auc: 0.8363]
INFO:root:[  3/100] T 599/2704 [WeightedBalanceLoss: 2.6526][loss: 2.6526][auc: 0.8375]
INFO:root:[  3/100] T 699/2704 [WeightedBalanceLoss: 2.4368][loss: 2.4368][auc: 0.8362]
INFO:root:[  3/100] T 799/2704 [WeightedBalanceLoss: 3.7221][loss: 3.7221][auc: 0.8378]
INFO:root:[  3/100] T 899/2704 [WeightedBalanceLoss: 2.5249][loss: 2.5249][auc: 0.8385]
INFO:root:[  3/100] T 999/2704 [WeightedBalanceLoss: 2.6413][loss: 2.6413][auc: 0.8382]
INFO:root:[  3/100] T 1099/2704 [WeightedBalanceLoss: 2.3836][loss: 2.3836][auc: 0.8387]
INFO:root:[  3/100] T 1199/2704 [WeightedBalanceLoss: 2.8426][loss: 2.8426][auc: 0.8397]
